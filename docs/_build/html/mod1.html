
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Module 1: The Basics of Forward Mode &#8212; Auto-eD  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Module 2: Deeper Into Forward Mode" href="mod2.html" />
    <link rel="prev" title="An Introduction to Automatic Differentiation with a Visualization Tool" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-1-the-basics-of-forward-mode">
<h1>Module 1: The Basics of Forward Mode<a class="headerlink" href="#module-1-the-basics-of-forward-mode" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Differentiation is fundamental to computational science and is important in many applications, including optimization,
sensitivity analysis, and solving differential equations. To be useful in these applications, derivatives must be computed
both precisely and efficiently. <strong>Automatic differentiation</strong>, sometimes also called algorithmic differentiation or
computational differentiation, can efficiently compute derivatives to machine precision, distinguishing it from both
numerical differentiation and symbolic differentiation. To kick things off, we will discuss the differences between automatic
differentiation and numerical and symbolic differentiation.</p>
<ul class="simple">
<li>Automatic differentiation is not numerical differentiation.</li>
</ul>
<p><em>Numerical differentiation</em> refers to a class of methods that computes derivatives through finite difference formulae based
on the definition of the derivative,</p>
<div class="math notranslate nohighlight">
\[\frac{df(x)}{dx} \approx \frac{f(x+h)-f(x)}{h}.\]</div>
<p>Such methods are limited in precision due to truncation and roundoff errors because  accuracy depends on choosing an
appropriate step size <cite>h</cite>. Let’s consider a basic example.</p>
<div class="section" id="demo-1-errors-in-the-finite-difference-method">
<h3>Demo 1: Errors in The Finite Difference Method<a class="headerlink" href="#demo-1-errors-in-the-finite-difference-method" title="Permalink to this headline">¶</a></h3>
<p>Let’s consider the function <span class="math notranslate nohighlight">\(f(x) = x-\exp(-2\sin^2(4x))\)</span>. Using our basic differentiation rules, we can compute the
derivative symbolically,</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dx} = 1 + 16\exp(-2\sin^2(4x))\sin(4x)\cos(4x).\]</div>
<p>Let’s write some code to calculate derivatives using the finite difference method for this function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#define our function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1">#explicitly define the derivative to compare accuracy</span>
<span class="k">def</span> <span class="nf">dfdx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">+</span><span class="mi">16</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="c1">#get numerical derivative at x for stepsize h</span>
<span class="k">def</span> <span class="nf">finite_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">h</span>

<span class="c1">#explore accuracy when changing h</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">hs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">13</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">errs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hs</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hs</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">finite_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">dfdx</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># compute error at each domain point</span>
    <span class="n">errs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="c1"># store L2 norm of error</span>

<span class="c1">#make plot of the error</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hs</span><span class="p">,</span> <span class="n">errs</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;h&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\|f^{\prime}_</span><span class="si">{FD}</span><span class="s1">-f^{\prime}_</span><span class="si">{exact}</span><span class="s1">\|_</span><span class="si">{L_2}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<p>The code above produces the following plot, showing the effects of the choice of <cite>h</cite> on the accuracy of the finite difference
method.</p>
<img alt="_images/hEffect.png" src="_images/hEffect.png" />
<p>In the plot above, we see that the accuracy of the derivative calculation is highly dependent on our choice of <cite>h</cite>.  When we
choose <cite>h</cite> too large, the numerical approximation is no longer accurate, but for <cite>h</cite> too small, we begin to see round off
errors from limitations in machine precision. We can do a back of the envelope calculation to see the main idea in action.
Say you choose <span class="math notranslate nohighlight">\(h = 10^{-12}\)</span>, which isn’t machine precision, but it’s pretty close. Now, the numerator of the finite
difference formula is <cite>f(x + h) - f(x)</cite>. The smallest this difference could be would be machine precision, <span class="math notranslate nohighlight">\(h_m
\approx 10^{-16}\)</span>, due to floating point errors. Suppose this is the case. Then, when dividing that difference by the
selected <cite>h</cite>, there is an error of <span class="math notranslate nohighlight">\(10^{-4}\)</span>! This is pretty far away from machine precision and it happened because
<cite>h</cite> is too small.</p>
<p>See Exercise 1 for another example motivating the use of automatic differentiation.</p>
<p>Now let’s discuss symbolic differentiation.</p>
<ul class="simple">
<li>Automatic differentiation is not symbolic differentiation.</li>
</ul>
<p><em>Symbolic differentiation</em> computes exact expressions for derivatives using expression trees. As seen in the function in Demo
1, exact expressions for derivatives can quickly become complex, which can somtimes make computing derivatives in this manner
computationally inefficient.</p>
<ul class="simple">
<li><strong>Automatic differentiation is a procedure that computes derivatives to machine precision without explicitly forming an
expression for the derivative. It relies on application of the ideas of the chain rule to decompose complex functions into
elementary functions for which we can compute the derivative exactly.</strong></li>
</ul>
<p>Automatic differentiation can be split into two primary “modes”: forward and reverse. Both modes allow for efficient and
accurate computation of derivatives. Advanced approaches exist for combining forward and reverse mode into a hybrid, but
these are beyond the scope of these introductory modules. The properties of automatic differentiation make it useful for a variety of
applications including machine learning, parameter optimization, sensitivity analysis, physical modeling, and probabilistic
inference. In the rest of this module, we will begin to explore the mechanics of automatic differentiation.</p>
</div>
</div>
<div class="section" id="the-basics-of-forward-mode">
<h2>The Basics of Forward Mode<a class="headerlink" href="#the-basics-of-forward-mode" title="Permalink to this headline">¶</a></h2>
<p>Automatic differentiation is something of a compromise between numerical differentiation and symbolic differentiation.
Automatic differentiation provides numerical values of the derivatives of a function. These numerical values are correct to
machine precision and are not influenced by any kind of “step size” like in numerical differentiation. Even though automatic
differentiation provides derivatives to machine precision, it does not require evaluation of a symbolic derivative. One other
thing to keep in mind about automatic differentation is that we usually think of it as yielding the derivative of a function
evaluated at a specific point. This should be borne in mind throughout this module. We will evaluate a function at a specific
point and we will automatically get its derivative at that same point.</p>
<p>The major concept underlying automatic differentiation is <em>the chain rule</em>. Recall from calculus that the chain rule states
that to find the derivative of a composition of functions, we multiply a series of derivatives. For illustration, let
<span class="math notranslate nohighlight">\(f(t) = g(h(t))\)</span>. We have</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dt} = \frac{dg}{dh}\frac{dh}{dt}\]</div>
<p>This can be generalized to functions of multiple inputs, which we will discuss in more detail in Module 2.</p>
<div class="section" id="elementary-functions">
<h3>Elementary Functions<a class="headerlink" href="#elementary-functions" title="Permalink to this headline">¶</a></h3>
<p>Every function can be decomposed into a set of binary elementary operations or unary elementary
functions. Elementary operations include addition, subtraction, multiplication, division, and exponentiation. Elementary
functions include the natural exponential and natural logarithm, trigonometric functions, and polynomials. The sigmoid
function and the hyperbolic trig functions can also be considered elementary functions, though they can be formed from the
natural exponential.</p>
<p>Basic calculus provides closed form differentiation rules for these elementary functions. This means that we can compose
these functions to form more complex functions and find the derivative of these more complex functions using the chain rule.
<em>This is the key idea behind automatic differentiation</em>. We know the derivatives of the elementary functions. Complicated
functions are composed of elementary functions. The chain rule provides a route to calculating derivatives of functions that
are composed of other functions.</p>
<p>To understand this composition of elementary functions, we can think of the composition of functions as having an underlying
graph structure. You will learn much more about this graph structure in Module 2, including a way to build it by hand. For now,
you will practice visualizing the graph with a special tool.</p>
</div>
</div>
<div class="section" id="a-tool-for-visualizing-automatic-differentiation">
<h2>A Tool for Visualizing Automatic Differentiation<a class="headerlink" href="#a-tool-for-visualizing-automatic-differentiation" title="Permalink to this headline">¶</a></h2>
<p>The Auto-eD tool is a pedagogical tool to help visualize the processes underlying automatic differentiation. In particular,
this tool allows us to visualize the underlying graph structure of a calculation when decomposed into elementary functions.
In addition to helping to visualize this graph, the tool can also be used to view the computational traces that occur at each
node of the graph. These ideas will be discussed much more in Module 2.</p>
<div class="section" id="auto-ed-web-application">
<h3>Auto-eD Web Application<a class="headerlink" href="#auto-ed-web-application" title="Permalink to this headline">¶</a></h3>
<p>The tool can be accessed directly through a web browser by visiting <a class="reference external" href="https://autoed.herokuapp.com">https://autoed.herokuapp.com</a>. This option is good for
people who want to explore automatic differentiation.</p>
</div>
<div class="section" id="developer-instructions">
<h3>Developer Instructions<a class="headerlink" href="#developer-instructions" title="Permalink to this headline">¶</a></h3>
<p>Auto-eD is open source. You are free to check out the code and even contribute improvements. To run the tool with the ability
to modify and contribute to the code, you may choose to clone the Github repo to have direct access to the code for the web
app and access to the underlying package. From the terminal,</p>
<ol class="arabic simple">
<li>Clone the repo:: git clone <a class="reference external" href="https:github.com/lindseysbrown/Auto-eD">https:github.com/lindseysbrown/Auto-eD</a></li>
<li>Install the dependencies:: pip install -r requirements.txt</li>
<li>Launch the web app from the terminal:: python ADapp.py</li>
<li>Visit <a class="reference external" href="http://0.0.0.0:5000">http://0.0.0.0:5000</a> to use the tool with your local server.</li>
</ol>
<p>We welcome improvements and contributions! You can find more details about the underlying package in the DeveloperDocumentation jupyter notebook.  If you would like to contribute to this project, please follow these steps:</p>
<ol class="arabic simple">
<li>Clone the repo</li>
<li>Create a new branch with an informative branch name</li>
<li>Make sure all your updates are on the new branch</li>
<li>Make a pull request to master and wait for the core developers to respond!</li>
</ol>
</div>
</div>
<div class="section" id="a-first-demo-of-automatic-differentiation">
<h2>A First Demo of Automatic Differentiation<a class="headerlink" href="#a-first-demo-of-automatic-differentiation" title="Permalink to this headline">¶</a></h2>
<p>Let’s use the tool to visualize the function from our first demo. The example below was done using the web app.</p>
<ol class="arabic simple">
<li>The function has a single input variable, <cite>x</cite>, so just enter 1 in the “Number of input variables” field.</li>
<li>Our function is scalar valued so we enter that our function has 1 output.</li>
</ol>
<img alt="_images/Step1.PNG" src="_images/Step1.PNG" />
<p>3. We use the calculator interface to enter our function. (Use the backspace key or the “Clear All” button
to correct the function if we make a mistake when entering it.) With the current release, you must click on the functions on
the calculator rather than entering them from the keyboard.</p>
<img alt="_images/Step2.PNG" src="_images/Step2.PNG" />
<p>4. Press the “Calculate” button.  This will move you to a new screen with options to help you visualize both the forward and
reverse mode of automatic differentiation.</p>
<p>5. Enter the value for x at which you’d like to evaluate the function. For the purposes of this demo, we’ll choose <cite>x=4</cite>.
Click  the “Set Input Values” button.</p>
<blockquote>
<div><ul class="simple">
<li>Note that automatic differentiation yields the <em>value</em> of the derivative at a specific point. It does not compute a
symbolic expression for the derivative.</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="6">
<li>You’ll see the values for the function and derivative appear beside your function and input values you selected.</li>
</ol>
<img alt="_images/Step3.PNG" src="_images/Step3.PNG" />
<p>7. Below this, you’ll see buttons for which function you’d like to visualize. In this example, we only have a single
function, so click on f1.</p>
<p>8. This will generate the computational graph for both forward and reverse mode as well as the computational trace table.
We’ll talk more about the computational table and reverse mode in the next modules, so for now let’s just focus on the
computational graph in forward mode.</p>
<img alt="_images/Step4.PNG" src="_images/Step4.PNG" />
<p>9. The single magenta node represents the input to the function. The single green output node represents the output value of
our function. The red nodes represent intermediate function values. Notice that all of the nodes are connected by elementary
operations on the labelled edges.</p>
<blockquote>
<div><ul class="simple">
<li>(Hint: Occasionally the graphs may be difficult to read depending
on the complexity of the function that you are visualizing. You can try running the tool a second time to get a different
configuration of the nodes. Alternatively, for large functions, you can run the package from the command line, which will
generate graphs that you can maximize to resize the edges.)</li>
</ul>
</div></blockquote>
<div class="section" id="some-key-takeaways">
<h3>Some Key Takeaways<a class="headerlink" href="#some-key-takeaways" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Our function was decomposed into a series of elementary operations.</li>
<li>These operations include both basic binary operations (addition, subtraction, multiplication, and division), unary
operations (negation), and elementary functions (exponential functions, trigonometric functions).</li>
<li>Using this graph to compute the derivative is the same process as using the chain rule to compute the derivative, allowing
the derivative to be computed to machine precision.</li>
<li>Don’t worry if you don’t understand this perfectly yet. At this time, you should appreciate that automatic differentiation
gives the exact value of the derivative at a specified point. The graph displayed by the tool is a representation of the
function itself and depicts how the function is built up from elementary functions.</li>
</ul>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="exercise-1-motivating-automatic-differentiation">
<h3>Exercise 1: Motivating Automatic Differentiation<a class="headerlink" href="#exercise-1-motivating-automatic-differentiation" title="Permalink to this headline">¶</a></h3>
<ol class="upperalpha">
<li><p class="first">Write a Python function that takes two inputs: 1. a function (of a single variable) and 2. a value of <cite>h</cite>. This function
should return a function which has a single input: a value of <cite>x</cite>.  This inner function should compute the numerical
approximation of the derivative of <cite>f</cite> with stepsize <cite>h</cite> at <cite>x</cite>.</p>
<blockquote>
<div><ul class="simple">
<li>Note: This part of the exercise is meant to be implemented as a closure in Python. It consists of an outer function and
an inner function.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Let <span class="math notranslate nohighlight">\(f(x) = ln(x)\)</span>. For <span class="math notranslate nohighlight">\(0.2\leq x \leq 0.4\)</span>, make a plot comparing the numerically estimated derivative for
<span class="math notranslate nohighlight">\(h=10^{-1}, h=10^{-7}\)</span>, and <span class="math notranslate nohighlight">\(h=10^{-15}\)</span> to the analytic derivative (which should be used explicitly).</p>
<blockquote>
<div><ul class="simple">
<li>Note: All plots should be on the same figure. This means there should be 4 lines, three for the different values of <cite>h</cite>
and one for the true solution. Make sure to include a legend and that the different lines are distinguishable.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Answer the following questions:</p>
<ul class="simple">
<li>Which value of <cite>h</cite> most closely approximates the true derivative? What happens for values of <cite>h</cite> that are too small?  What
happens for values of <cite>h</cite> that are too large?</li>
<li>How does automatic differentiation address these problems?</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="exercise-2-basic-graph-structure-of-calculations">
<h3>Exercise 2: Basic Graph Structure of Calculations<a class="headerlink" href="#exercise-2-basic-graph-structure-of-calculations" title="Permalink to this headline">¶</a></h3>
<p>Consider the function <span class="math notranslate nohighlight">\(f(x)= \tan(x^2+3)+x\)</span>.</p>
<p>Draw the graph with the visualization tool.</p>
</div>
<div class="section" id="exercise-3-looking-toward-multiple-inputs">
<h3>Exercise 3: Looking Toward Multiple Inputs<a class="headerlink" href="#exercise-3-looking-toward-multiple-inputs" title="Permalink to this headline">¶</a></h3>
<p>We can use the same process to compute derivatives for functions of multiple inputs.  Consider the function,</p>
<div class="math notranslate nohighlight">
\[f(x,y)=\exp(-(\sin(x)-\cos(y))^2)\]</div>
<p>Practice drawing the computational graph for this function using the visualization tool. We’ll discuss the theory behind
functions of multiple inputs in the next module.</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Auto-eD</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Module 1: The Basics of Forward Mode</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-basics-of-forward-mode">The Basics of Forward Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a-tool-for-visualizing-automatic-differentiation">A Tool for Visualizing Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a-first-demo-of-automatic-differentiation">A First Demo of Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mod2.html">Module 2: Deeper Into Forward Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="mod3.html">Module 3: The Reverse Mode of Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mod4.html">Beyond the Basics: Extensions and Software Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="refs.html">References and Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="sols.html">Solutions to Exercises</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">An Introduction to Automatic Differentiation with a Visualization Tool</a></li>
      <li>Next: <a href="mod2.html" title="next chapter">Module 2: Deeper Into Forward Mode</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Lindsey Brown and David Sondak.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/mod1.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>